{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Medical appointments are time commitments doctors make with patients. However, some people do not show up for different reasons, that cause lost time and money for the doctor. Let's build models that predict whether the next appointment is a show or no show!\n",
    "License: the dataset is CC4.0: BY-NC-SA, and it is publicly available online.\n",
    "You will need to use GitHub to complete this mini project. Find Guidelines of Using GitHub Here.\n",
    "Expected Output\n",
    "By the end of this mini project, you are supposed to deliver within your code:\n",
    "Multiple accuracy measures resembling different criteria used for training your decision tree classifiers.\n",
    "Multiple accuracy measures resembling a different number of estimators used for your random forest classifiers.\n",
    "One printed confusion matrix for the best model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# STEP 1: Download the Dataset\n",
    "# STEP 2: Reading the Dataset. Does the dataset include any missing values? If so, drop them!\n",
    "dataset = pd.read_csv('KaggleV2-May-2016.csv')\n",
    "print(\"DATASET\", dataset)\n",
    "print(\"DATASET INFO\", dataset.info())\n",
    "\n",
    "\"\"\"nissing values elimination is optional. don't have to have acomplete row because we are not passing arrays. it can pass over missing values\"\"\"\n",
    "missing_mask = dataset.isna()\n",
    "print(\"BOOLEAN MISSING VALUE MASK:\")\n",
    "print(missing_mask.head())  # Shows True where data is missing\n",
    "\n",
    "# Step 2b: Count total missing values per column\n",
    "missing_counts = missing_mask.sum()\n",
    "print(\"\\nCOUNT OF MISSING VALUES PER COLUMN:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Step 2c: Count total missing values in entire dataset\n",
    "total_missing = missing_mask.values.sum()\n",
    "print(\"\\nTOTAL MISSING VALUES IN DATASET:\", total_missing)\n",
    "\n",
    "\"\"\"\n",
    "no missing valus found\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print('SHAPE', dataset.shape)\n",
    "print(\"NUMBER OF TUPLES\",(dataset.shape)[0])  \n",
    "\n",
    "\n",
    "# Create a list of the features we want to keep and check numerical vs categorical\n",
    "\n",
    "\"\"\"\n",
    "numerical_cols = ['PatientId','AppointmentID', 'Age', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received']\n",
    "categorical_cols = ['Gender','Neighbourhood','No-show']\n",
    "date_time_column = ['ScheduledDay', 'AppointmentDay']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "selected_features = [\n",
    "     \n",
    "    \"Gender\",\n",
    "    \"Age\",\n",
    "    \"Scholarship\",\n",
    "    \"Hipertension\",\n",
    "    \"Diabetes\",\n",
    "    \"Alcoholism\",\n",
    "    \"Handcap\",\n",
    "    \"SMS_received\"\n",
    "    \n",
    "]\n",
    "\n",
    "# # Create a new DataFrame with only the selected columns\n",
    "df_subset = dataset[selected_features]\n",
    "\n",
    "\n",
    "print(df_subset.head())\n",
    "print(f\"New DataFrame shape: {df_subset.shape}\")\n",
    "\n",
    "# Step 4: Preprocessing\n",
    "\"\"\"\n",
    "Perform any needed pre-processing on the chosen features, includes:\n",
    "Scaling\n",
    "Encoding\n",
    "Dealing with NaN values\n",
    "Note:\n",
    "Use only the preprocessing steps you think are useful.\n",
    "\n",
    "\"\"\"\n",
    "#  Encode target column No-show\n",
    "# Convert 'No-show' to 0 and 1\n",
    "# label encoder is not appropriate because it will convert No-show yes -> 0 and No -> 1 since it is alphabetical order\n",
    "dataset['No-show'] = dataset['No-show'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "numerical_cols = ['Age', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received']\n",
    "categorical_cols = ['Gender']\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = dataset[selected_features]\n",
    "y = dataset['No-show']\n",
    "print(\"label..\", y)\n",
    "\n",
    "# Encoding (One-Hot Encoding) - recommended over others for multiple features\n",
    "# Creates new binary columns from the categorical features\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "X_encoded = X_encoded.astype(int) #to avoid True False after encoding\n",
    "print(\"features_encoded:\",X_encoded)\n",
    "\n",
    "\"\"\"\n",
    "Decisin Treee doesn't really care about scaling. The magnitide of numbers have a lot less influence on th emodel. For KNN\"and SVM it is needed\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# # Scaling (Standard Scaling)\n",
    "# scaler = StandardScaler()\n",
    "# # Apply scaler only to the numerical columns\n",
    "# X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])\n",
    "\n",
    "# Final Data Assembly and Inspection\n",
    "# Combine processed features and target for the final model-ready DataFrame\n",
    "df_processed_final = pd.concat([X_encoded, y], axis=1)\n",
    "\n",
    "print(\"\\n--- Final Preprocessing Summary ---\")\n",
    "print(f\"Final DataFrame shape: {df_processed_final.shape}\")\n",
    "print(f\"Number of features after One-Hot Encoding and Scaling: {df_processed_final.shape[1] - 1}\")\n",
    "print(\"\\nFirst 5 rows of the fully processed DataFrame (Note the scaled and encoded values):\")\n",
    "print(df_processed_final.head())\n",
    "print(\"\\nFinal DataFrame Info:\")\n",
    "df_processed_final.info()\n",
    "\n",
    "# Step 5:  Splitting the Data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Split your data as follows:\n",
    "80% training set\n",
    "10% validation set\n",
    "10% test set\n",
    "using the 80/10/10 split as it provides a dedicated Validation Set for fine-tuning without touching the final Test Set.\n",
    "\n",
    "\"\"\"\n",
    "# Assuming X and y are the features and target\n",
    "\n",
    "# Create Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Step 2: Split Temp (20%) into 10% Validation and 10% Test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=1\n",
    ")\n",
    "\n",
    "# Choose the model\n",
    "model_tree = DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                    max_leaf_nodes = 10)\n",
    "# Train the model\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate the model\n",
    "y_pred_tree = model_tree.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred_tree))\n",
    "print(classification_report(y_test,y_pred_tree))\n",
    "\n",
    "# ----------------------------\n",
    "# TRY DIFFERENT DECISION TREE CRITERIA\n",
    "# ----------------------------\n",
    "\n",
    "criteria = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "val_scores = {}\n",
    "\n",
    "for c in criteria:\n",
    "    model = DecisionTreeClassifier(criterion=c)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    val_scores[c] = acc\n",
    "    print(f\"Criterion = {c:8s} | Validation Accuracy = {acc:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# SELECT BEST CRITERION\n",
    "# ----------------------------\n",
    "\n",
    "best_criterion = max(val_scores, key=val_scores.get)\n",
    "print(\"\\nBest Criterion:\", best_criterion)\n",
    "\n",
    "\"\"\"\n",
    "Random Forest\n",
    "Repeat step 6.\n",
    "Increase/decrease the number of estimators in random forest and comment on the difference of the classification metrics.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_RF = RandomForestClassifier(n_estimators = 10, # Default is 100, but we have small dataset\n",
    "                                  bootstrap = True) # If false, then all samples will be used in each tree \n",
    "\n",
    "model_RF.fit(X_train, y_train)\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "model_RF.score(X_test, y_test)\n",
    "\n",
    "print(confusion_matrix(y_test,      y_pred_RF))\n",
    "print(classification_report(y_test, y_pred_RF))\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators = 10, # Default is 100\n",
    "                                  criterion = \"gini\", # Can use {“gini”, “entropy”, “log_loss”}\n",
    "                                  bootstrap = True, # True = use a subset of samples, False = use all samples\n",
    "                                  max_features = 'sqrt', # This is the number of randomly chosen features to decide between at each split\n",
    "                                  class_weight = \"balanced\") # or a dictionary with key:value pair set-up: {class_label: weight}\n",
    "\n",
    "model_RF.fit(X_train, y_train)\n",
    "model_RF.score(X_test, y_test)\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "\n",
    "for n in n_estimators_list:\n",
    "    model_rf = RandomForestClassifier(\n",
    "        n_estimators=n,\n",
    "        random_state=1\n",
    "    )\n",
    "    \n",
    "    model_rf.fit(X_train, y_train)\n",
    "    y_pred = model_rf.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\n==== Random Forest with {n} estimators ====\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
